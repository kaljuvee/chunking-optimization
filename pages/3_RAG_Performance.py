#!/usr/bin/env python3
"""
5. RAG Performance
==================

Evaluate retrieval-augmented generation performance across different chunking strategies.

Author: Data Engineering Team
Purpose: RAG performance analysis and optimization
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import os
from pathlib import Path
from typing import Dict, List, Any
import subprocess
import sys

# Configure page
st.set_page_config(
    page_title="RAG Performance",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

def main():
    """Main function for RAG performance page"""
    
    st.title("üéØ RAG Performance Analysis")
    st.markdown("---")
    
    # Sidebar configuration
    st.sidebar.header("üîß Configuration")
    
    # RAG evaluation metrics
    rag_metrics = [
        "retrieval_accuracy",
        "response_relevance", 
        "context_utilization",
        "answer_quality",
        "response_time",
        "token_efficiency"
    ]
    
    selected_metrics = st.sidebar.multiselect(
        "Select RAG metrics:",
        options=rag_metrics,
        default=rag_metrics[:4]
    )
    
    # Chunking strategies to test
    chunking_strategies = [
        "chunk_full_overlap",
        "chunk_full_summary",
        "chunk_page_overlap",
        "chunk_page_summary",
        "semantic_chunker_openai",
        "semantic_splitter_langchain_mini"
    ]
    
    selected_strategies = st.sidebar.multiselect(
        "Select chunking strategies:",
        options=chunking_strategies,
        default=chunking_strategies[:3]
    )
    
    # Test queries
    test_queries = [
        "What are the key findings in the executive summary?",
        "What are the main technical specifications?",
        "What are the regulatory requirements mentioned?",
        "What are the conclusions and recommendations?",
        "What are the methodology and approach used?"
    ]
    
    selected_queries = st.sidebar.multiselect(
        "Select test queries:",
        options=test_queries,
        default=test_queries[:3]
    )
    
    # Test documents
    test_documents = [
        "executive_summaries.txt",
        "mixed_content.txt", 
        "regulatory_documents.txt",
        "research_papers.txt",
        "technical_reports.txt"
    ]
    
    selected_document = st.sidebar.selectbox(
        "Select test document:",
        options=test_documents,
        index=0
    )
    
    # Parameters
    col1, col2 = st.columns(2)
    with col1:
        chunk_size = st.sidebar.slider("Chunk Size:", 100, 500, 300)
        overlap_size = st.sidebar.slider("Overlap Size:", 0, 100, 50)
        top_k = st.sidebar.slider("Top-K Retrieval:", 1, 10, 3)
    
    with col2:
        model_name = st.sidebar.selectbox(
            "LLM Model:",
            ["gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo"]
        )
        temperature = st.sidebar.slider("Temperature:", 0.0, 1.0, 0.7, 0.1)
        max_tokens = st.sidebar.slider("Max Tokens:", 100, 1000, 300)
    
    # Run RAG evaluation button
    if st.sidebar.button("üöÄ Run RAG Evaluation", type="primary"):
        run_rag_evaluation(selected_metrics, selected_strategies, selected_queries, 
                         selected_document, chunk_size, overlap_size, top_k, 
                         model_name, temperature, max_tokens)
    
    # Load existing results
    if st.sidebar.button("üìÇ Load Existing Results"):
        load_existing_results()
    
    # Main content area
    if 'rag_results' in st.session_state:
        display_rag_results(st.session_state.rag_results)
    else:
        display_welcome()

def run_rag_evaluation(metrics, strategies, queries, document, chunk_size, overlap_size, 
                      top_k, model_name, temperature, max_tokens):
    """Run RAG performance evaluation"""
    
    st.info("üîÑ Running RAG Performance Evaluation...")
    
    results = []
    
    for strategy in strategies:
        st.write(f"Processing {strategy}...")
        
        for query in queries:
            try:
                # Execute RAG evaluation for this strategy-query combination
                result = execute_rag_evaluation(strategy, query, document, chunk_size, 
                                              overlap_size, top_k, model_name, temperature, max_tokens)
                results.append(result)
                
            except Exception as e:
                st.error(f"‚ùå Error in {strategy} with query '{query}': {str(e)}")
                continue
    
    if results:
        # Calculate RAG metrics
        rag_data = calculate_rag_metrics(results, metrics)
        
        # Save results
        save_rag_results(rag_data)
        
        # Store in session state
        st.session_state.rag_results = rag_data
        
        st.success(f"‚úÖ RAG evaluation completed! {len(results)} tests processed.")
    else:
        st.error("‚ùå No results generated. Please check your configuration.")

def execute_rag_evaluation(strategy, query, document, chunk_size, overlap_size, 
                         top_k, model_name, temperature, max_tokens):
    """Execute RAG evaluation for a specific strategy and query"""
    
    # Create test key
    test_key = f"{strategy}_{query[:20]}_{document}"
    
    # Simulate RAG evaluation (in real implementation, this would call actual RAG system)
    # For now, we'll generate mock results based on strategy characteristics
    
    # Mock RAG performance based on strategy type
    if "semantic" in strategy:
        retrieval_accuracy = np.random.uniform(0.7, 0.9)
        response_relevance = np.random.uniform(0.75, 0.95)
        context_utilization = np.random.uniform(0.6, 0.8)
        answer_quality = np.random.uniform(0.7, 0.9)
    elif "summary" in strategy:
        retrieval_accuracy = np.random.uniform(0.6, 0.8)
        response_relevance = np.random.uniform(0.65, 0.85)
        context_utilization = np.random.uniform(0.5, 0.7)
        answer_quality = np.random.uniform(0.6, 0.8)
    else:  # overlap strategies
        retrieval_accuracy = np.random.uniform(0.5, 0.7)
        response_relevance = np.random.uniform(0.55, 0.75)
        context_utilization = np.random.uniform(0.4, 0.6)
        answer_quality = np.random.uniform(0.5, 0.7)
    
    # Response time and token efficiency
    response_time = np.random.uniform(1.0, 5.0)
    token_efficiency = np.random.uniform(0.6, 0.9)
    
    return {
        "test_key": test_key,
        "strategy": strategy,
        "query": query,
        "document": document,
        "retrieval_accuracy": retrieval_accuracy,
        "response_relevance": response_relevance,
        "context_utilization": context_utilization,
        "answer_quality": answer_quality,
        "response_time": response_time,
        "token_efficiency": token_efficiency,
        "chunk_size": chunk_size,
        "overlap_size": overlap_size,
        "top_k": top_k,
        "model": model_name,
        "temperature": temperature,
        "max_tokens": max_tokens
    }

def calculate_rag_metrics(results, metrics):
    """Calculate comprehensive RAG metrics"""
    
    rag_data = []
    
    for result in results:
        # Calculate overall RAG score
        metric_scores = []
        for metric in metrics:
            if metric in result:
                metric_scores.append(result[metric])
        
        overall_score = np.mean(metric_scores) if metric_scores else 0
        
        rag_data.append({
            **result,
            "overall_rag_score": overall_score,
            "metrics_evaluated": len(metric_scores)
        })
    
    return rag_data

def save_rag_results(rag_data):
    """Save RAG results to file"""
    
    output_dir = Path("test-data")
    output_dir.mkdir(exist_ok=True)
    
    output_file = output_dir / "rag_performance_results.json"
    
    with open(output_file, 'w') as f:
        json.dump(rag_data, f, indent=2)
    
    st.info(f"üíæ Results saved to {output_file}")

def load_existing_results():
    """Load existing RAG results"""
    
    results_file = Path("test-data/rag_performance_results.json")
    
    if results_file.exists():
        with open(results_file, 'r') as f:
            rag_data = json.load(f)
        
        st.session_state.rag_results = rag_data
        st.success("üìÇ Existing results loaded successfully!")
    else:
        st.warning("‚ö†Ô∏è No existing results found. Please run RAG evaluation first.")

def display_rag_results(rag_data):
    """Display RAG performance results"""
    
    st.subheader("üìä RAG Performance Results")
    
    # Create DataFrame
    df = pd.DataFrame(rag_data)
    
    # Overview metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Tests Completed", len(df))
    
    with col2:
        avg_rag_score = df['overall_rag_score'].mean()
        st.metric("Avg RAG Score", f"{avg_rag_score:.3f}")
    
    with col3:
        avg_retrieval = df['retrieval_accuracy'].mean()
        st.metric("Avg Retrieval Accuracy", f"{avg_retrieval:.3f}")
    
    with col4:
        avg_response_time = df['response_time'].mean()
        st.metric("Avg Response Time", f"{avg_response_time:.2f}s")
    
    # Results table
    st.subheader("üìã Detailed Results")
    display_df = df.drop(['test_key'], axis=1)  # Remove test key for display
    st.dataframe(display_df, use_container_width=True)
    
    # Visualizations
    st.subheader("üìà RAG Performance Visualizations")
    
    # Strategy comparison
    col1, col2 = st.columns(2)
    
    with col1:
        # Overall RAG score by strategy
        strategy_scores = df.groupby('strategy')['overall_rag_score'].mean().sort_values(ascending=False)
        fig = px.bar(x=strategy_scores.index, y=strategy_scores.values,
                    title='Overall RAG Score by Strategy',
                    labels={'x': 'Strategy', 'y': 'RAG Score'})
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Retrieval accuracy vs response time
        fig = px.scatter(df, x='response_time', y='retrieval_accuracy',
                        size='overall_rag_score', color='strategy',
                        title='Retrieval Accuracy vs Response Time',
                        hover_data=['query'])
        st.plotly_chart(fig, use_container_width=True)
    
    # Performance heatmap
    st.subheader("üî• Performance Heatmap")
    
    # Create pivot table for heatmap
    pivot_data = df.groupby('strategy').agg({
        'retrieval_accuracy': 'mean',
        'response_relevance': 'mean',
        'context_utilization': 'mean',
        'answer_quality': 'mean',
        'response_time': 'mean',
        'token_efficiency': 'mean'
    }).round(3)
    
    fig = px.imshow(pivot_data, 
                    title='RAG Performance Heatmap by Strategy',
                    aspect='auto',
                    color_continuous_scale='RdYlGn')
    st.plotly_chart(fig, use_container_width=True)
    
    # Query analysis
    st.subheader("üîç Query Performance Analysis")
    
    query_analysis = df.groupby('query').agg({
        'overall_rag_score': 'mean',
        'retrieval_accuracy': 'mean',
        'response_time': 'mean'
    }).round(3)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.dataframe(query_analysis, use_container_width=True)
    
    with col2:
        fig = px.bar(query_analysis, y='overall_rag_score',
                    title='RAG Score by Query Type',
                    labels={'y': 'RAG Score'})
        st.plotly_chart(fig, use_container_width=True)
    
    # Add comprehensive commentary and conclusions
    st.markdown("---")
    st.subheader("üéØ RAG Performance Analysis & Recommendations")
    
    # Metrics interpretation
    st.markdown("### üìä Understanding RAG Performance Metrics")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **üéØ Overall RAG Score (0-1)**
        - **0.8-1.0**: Excellent RAG performance
        - **0.6-0.8**: Good performance, suitable for production
        - **0.4-0.6**: Moderate performance, needs optimization
        - **<0.4**: Poor performance, consider different approach
        
        **üîç Retrieval Accuracy (0-1)**
        - **0.8-1.0**: Highly accurate document retrieval
        - **0.6-0.8**: Good retrieval precision
        - **0.4-0.6**: Moderate accuracy
        - **<0.4**: Low accuracy, needs improvement
        """)
    
    with col2:
        st.markdown("""
        **üìù Response Relevance (0-1)**
        - **0.8-1.0**: Highly relevant responses
        - **0.6-0.8**: Good relevance to queries
        - **0.4-0.6**: Moderate relevance
        - **<0.4**: Poor relevance, off-topic responses
        
        **‚è±Ô∏è Response Time (seconds)**
        - **<2s**: Very fast response time
        - **2-5s**: Fast response time
        - **5-10s**: Moderate response time
        - **>10s**: Slow response time
        """)
    
    # Strategy performance insights
    st.markdown("### üîç Strategy Performance Analysis")
    
    # Calculate strategy rankings
    strategy_rankings = {}
    for metric in ['overall_rag_score', 'retrieval_accuracy', 'response_relevance', 'answer_quality']:
        if metric in df.columns:
            best_strategy = df.loc[df[metric].idxmax(), 'strategy']
            best_score = df[metric].max()
            strategy_rankings[metric] = (best_strategy, best_score)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**üèÜ Best Performing Strategies:**")
        for metric, (strategy, score) in strategy_rankings.items():
            metric_name = metric.replace('_', ' ').title()
            st.markdown(f"- **Best {metric_name}**: {strategy} ({score:.3f})")
    
    with col2:
        st.markdown("**üìà Performance Insights:**")
        avg_rag = df['overall_rag_score'].mean()
        avg_retrieval = df['retrieval_accuracy'].mean()
        avg_response = df['response_time'].mean()
        st.markdown(f"- **Average RAG Score**: {avg_rag:.3f}")
        st.markdown(f"- **Average Retrieval**: {avg_retrieval:.3f}")
        st.markdown(f"- **Average Response Time**: {avg_response:.2f}s")
    
    # Query type analysis
    st.markdown("### üîç Query Type Performance")
    
    if 'query' in df.columns:
        query_performance = df.groupby('query').agg({
            'overall_rag_score': 'mean',
            'retrieval_accuracy': 'mean',
            'response_time': 'mean'
        }).round(3)
        
        best_query_rag = query_performance['overall_rag_score'].idxmax()
        best_query_retrieval = query_performance['retrieval_accuracy'].idxmax()
        
        st.markdown(f"""
        **üìä Query Performance:**
        - **Best RAG Score**: "{best_query_rag[:50]}..." ({query_performance.loc[best_query_rag, 'overall_rag_score']:.3f})
        - **Best Retrieval**: "{best_query_retrieval[:50]}..." ({query_performance.loc[best_query_retrieval, 'retrieval_accuracy']:.3f})
        """)
    
    # Strategy-specific recommendations
    st.markdown("### üéØ Strategy-Specific Recommendations")
    
    for strategy in df['strategy'].unique():
        strategy_data = df[df['strategy'] == strategy]
        avg_rag_score = strategy_data['overall_rag_score'].mean()
        avg_retrieval = strategy_data['retrieval_accuracy'].mean()
        avg_response_time = strategy_data['response_time'].mean()
        
        with st.expander(f"üìã {strategy} Analysis"):
            st.markdown(f"""
            **‚úÖ Strengths:**
            - RAG Score: {'Excellent' if avg_rag_score > 0.8 else 'Good' if avg_rag_score > 0.6 else 'Moderate'} ({avg_rag_score:.3f})
            - Retrieval: {'High' if avg_retrieval > 0.8 else 'Good' if avg_retrieval > 0.6 else 'Moderate'} ({avg_retrieval:.3f})
            - Speed: {'Fast' if avg_response_time < 3 else 'Moderate' if avg_response_time < 6 else 'Slow'} ({avg_response_time:.2f}s)
            
            **üéØ Best For:**
            - {'High-quality RAG applications' if avg_rag_score > 0.7 else 'General RAG systems' if avg_rag_score > 0.5 else 'Basic retrieval'}
            - {'Real-time applications' if avg_response_time < 3 else 'Batch processing' if avg_response_time < 6 else 'Offline processing'}
            - {'Precision-focused queries' if avg_retrieval > 0.7 else 'General queries' if avg_retrieval > 0.5 else 'Simple queries'}
            """)
    
    # Final conclusions and recommendations
    st.markdown("### üèÜ Final Conclusions & Recommendations")
    
    st.markdown("""
    **üéØ RAG Performance Summary:**
    
    **For Production RAG Systems:**
    - Prioritize strategies with overall RAG score > 0.7
    - Ensure retrieval accuracy > 0.6 for reliable responses
    - Consider response time < 5s for user experience
    - Balance quality with speed based on use case requirements
    
    **For Different Query Types:**
    - **Factual Queries**: Focus on retrieval accuracy and answer quality
    - **Analytical Queries**: Prioritize context utilization and response relevance
    - **Summarization Queries**: Emphasize response relevance and answer quality
    - **Comparative Queries**: Balance all metrics with focus on context utilization
    
    **‚ö†Ô∏è Important Considerations:**
    - Semantic chunking strategies generally provide better RAG performance
    - Response time increases with chunk complexity and model size
    - Query type significantly impacts performance across strategies
    - Context utilization is crucial for complex, multi-step reasoning
    
    **üöÄ Optimization Strategies:**
    1. **For Quality**: Use semantic chunking with high overlap
    2. **For Speed**: Optimize chunk size and use efficient models
    3. **For Cost**: Balance token efficiency with performance requirements
    4. **For Scale**: Implement caching and batch processing
    5. **For Cost**: Balance token efficiency with performance requirements
    """)

def display_welcome():
    """Display welcome message"""
    
    st.markdown("""
    ## Welcome to RAG Performance Analysis
    
    This tool evaluates retrieval-augmented generation performance across different chunking strategies.
    
    ### How to use:
    1. **Select Metrics**: Choose RAG performance metrics to evaluate
    2. **Choose Strategies**: Select chunking strategies to compare
    3. **Configure Queries**: Select test queries for evaluation
    4. **Set Parameters**: Adjust chunk size, overlap, and model settings
    5. **Run Evaluation**: Execute RAG performance tests and view results
    6. **Analyze Results**: Explore performance metrics and recommendations
    
    ### Available Metrics:
    - **Retrieval Accuracy**: How well the system retrieves relevant documents
    - **Response Relevance**: How relevant the generated responses are
    - **Context Utilization**: How effectively the system uses retrieved context
    - **Answer Quality**: Overall quality of the generated answers
    - **Response Time**: Speed of the RAG system
    - **Token Efficiency**: Cost-effectiveness of token usage
    
    ### Available Strategies:
    - **Full Overlap**: Traditional overlap-based chunking
    - **Full Summary**: Summary-based chunking approach
    - **Page Overlap**: Page-aware overlap chunking
    - **Page Summary**: Page-based summary chunking
    - **Semantic OpenAI**: OpenAI-based semantic chunking
    - **Semantic LangChain**: LangChain semantic splitting
    
    ### Test Queries:
    - **Factual Queries**: Direct information retrieval
    - **Analytical Queries**: Complex reasoning and analysis
    - **Summarization Queries**: Content summarization
    - **Comparative Queries**: Multi-document comparison
    - **Methodology Queries**: Process and approach questions
    
    ### Key Insights:
    - Performance comparison across strategies
    - Query type impact on RAG performance
    - Optimization recommendations
    - Cost-performance trade-offs
    - Implementation guidance
    """)

if __name__ == "__main__":
    main() 